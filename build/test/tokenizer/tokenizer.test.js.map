{"version":3,"file":"tokenizer.test.js","sourceRoot":"","sources":["../../../test/tokenizer/tokenizer.test.ts"],"names":[],"mappings":";;AAAA,iBAAe;AAEf,+BAA4B;AAE5B,mDAAmD;AAEnD,QAAQ,CAAC,WAAW,EAAE,GAAG,EAAE;IACzB,QAAQ,CAAC,UAAU,EAAE,GAAG,EAAE;QACxB,EAAE,CAAC,8DAA8D,EAAE,GAAG,EAAE;YACtE,MAAM,QAAQ,GAAG,IAAI,GAAG,CAAC,EAAE,CAAC,CAAC;YAC7B,MAAM,SAAS,GAAG,IAAI,qBAAS,CAAC,QAAQ,CAAC,CAAC;YAC1C,MAAM,KAAK,GACP,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC;YAE3C,KAAK,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,KAAK,EAAE,EAAE;gBAC5B,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;gBACpB,MAAM,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;gBACrB,SAAS,CAAC,OAAO,CAAC,GAAG,EAAE,IAAI,CAAC,CAAC;gBAC7B,aAAM,CAAC,KAAK,CAAC,SAAS,CAAC,KAAK,CAAC,MAAM,EAAE,KAAK,GAAG,CAAC,CAAC,CAAC;gBAChD,aAAM,CAAC,KAAK,CAAC,SAAS,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,IAAI,CAAC,CAAC;gBAC3C,aAAM,CAAC,KAAK,CAAC,SAAS,CAAC,IAAI,CAAC,KAAK,CAAC,EAAE,GAAG,CAAC,CAAC;YAC3C,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QAEH,EAAE,CAAC,gDAAgD,EAAE,GAAG,EAAE;YACxD,MAAM,QAAQ,GAAG,IAAI,GAAG,CAAC,EAAE,CAAC,CAAC;YAC7B,MAAM,SAAS,GAAG,IAAI,qBAAS,CAAC,QAAQ,CAAC,CAAC;YAC1C,MAAM,KAAK,GAAG,yBAAyB,CAAC;YACxC,SAAS,CAAC,OAAO,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;YAC5B,MAAM,QAAQ,GAAG,SAAS,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAC1C,MAAM,QAAQ,GACV,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC,CAAC;YACrD,aAAM,CAAC,SAAS,CAAC,QAAQ,EAAE,QAAQ,CAAC,CAAC;QACvC,CAAC,CAAC,CAAC;QAEH,EAAE,CAAC,6BAA6B,EAAE,GAAG,EAAE;YACrC,MAAM,QAAQ,GAAG,IAAI,GAAG,CAAC,EAAE,CAAC,CAAC;YAC7B,MAAM,SAAS,GAAG,IAAI,qBAAS,CAAC,QAAQ,CAAC,CAAC;YAE1C,mEAAmE;YACnE,MAAM,KAAK,GAAG,CAAC,OAAO,EAAE,OAAO,EAAE,OAAO,CAAC,CAAC;YAE1C,KAAK,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,KAAK,EAAE,EAAE;gBAC5B,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;YACjC,CAAC,CAAC,CAAC;YAEH,4DAA4D;YAC5D,KAAK,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,KAAK,EAAE,EAAE;gBAC5B,aAAM,CAAC,KAAK,CAAC,SAAS,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC;gBACnD,aAAM,CAAC,KAAK,CAAC,SAAS,CAAC,YAAY,CAAC,KAAK,CAAC,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC;YAC5D,CAAC,CAAC,CAAC;YAEH,yCAAyC;YACzC,MAAM,KAAK,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC;YAC7C,MAAM,gBAAgB,GAAG;gBACvB,CAAC,CAAC,CAAC;gBACH,CAAC,CAAC,EAAE,CAAC,CAAC;gBACN,CAAC,CAAC,EAAE,CAAC,CAAC;gBACN,CAAC,CAAC,EAAE,CAAC,CAAC;gBACN,CAAC,CAAC,CAAC;gBACH,CAAC,CAAC,CAAC,CAAM,IAAI;aACd,CAAC;YAEF,MAAM,gBAAgB,GAClB,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACtE,aAAM,CAAC,SAAS,CAAC,gBAAgB,EAAE,gBAAgB,CAAC,CAAC;YAErD,4CAA4C;YAC5C,MAAM,mBAAmB,GAAG;gBAC1B,CAAC;gBACD,CAAC;gBACD,CAAC;gBACD,CAAC;gBACD,CAAC;gBACD,CAAC,CAAG,IAAI;aACT,CAAC;YACF,MAAM,mBAAmB,GAAG,KAAK,CAAC,GAAG,CACjC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,CAAC,eAAe,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACnE,aAAM,CAAC,SAAS,CAAC,mBAAmB,EAAE,mBAAmB,CAAC,CAAC;QAC7D,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,WAAW,EAAE,GAAG,EAAE;QACzB,EAAE,CAAC,2CAA2C,EAAE,GAAG,EAAE;YACnD,MAAM,QAAQ,GAAG,IAAI,GAAG,CAAC,EAAE,CAAC,CAAC;YAC7B,MAAM,SAAS,GAAG,IAAI,qBAAS,CAAC,QAAQ,CAAC,CAAC;YAC1C,MAAM,KAAK,GAAG,yDAAyD,CAAC;YACxE,MAAM,KAAK,GAAG,KAAK,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;YAC/B,MAAM,OAAO,GAAG,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;YAC9D,MAAM,QAAQ,GAAG,OAAO,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;YACnC,MAAM,QAAQ,GAAG,gDAAgD,CAAC;YAClE,aAAM,CAAC,KAAK,CAAC,QAAQ,EAAE,QAAQ,CAAC,CAAC;QACnC,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["import 'mocha';\n\nimport {assert} from 'chai';\n\nimport {PID, Tokenizer} from '../../src/tokenizer';\n\ndescribe('Tokenizer', () => {\n  describe('#addItem', () => {\n    it('should add item text to `this.items` and PIDs to `this.pids`', () => {\n      const badWords = new Set([]);\n      const tokenizer = new Tokenizer(badWords);\n      const items: Array<[PID, string]> =\n          [[1, 'one'], [2, 'two'], [3, 'three']];\n\n      items.forEach((item, index) => {\n        const pid = item[0];\n        const text = item[1];\n        tokenizer.addItem(pid, text);\n        assert.equal(tokenizer.items.length, index + 1);\n        assert.equal(tokenizer.items[index], text);\n        assert.equal(tokenizer.pids[index], pid);\n      });\n    });\n\n    it('should apply MurmurHash3 with seed value of 0.', () => {\n      const badWords = new Set([]);\n      const tokenizer = new Tokenizer(badWords);\n      const input = 'small unsweeten ice tea';\n      tokenizer.addItem(1, input);\n      const observed = tokenizer.hashedItems[0];\n      const expected: number[] =\n          [2557986934, 1506511588, 4077993285, 1955911164];\n      assert.deepEqual(observed, expected);\n    });\n\n    it('should build posting lists.', () => {\n      const badWords = new Set([]);\n      const tokenizer = new Tokenizer(badWords);\n\n      // DESIGN NOTE: the terms 'a'..'f' are known to stem to themselves.\n      const items = ['a b c', 'b c d', 'd e f'];\n\n      items.forEach((item, index) => {\n        tokenizer.addItem(index, item);\n      });\n\n      // Verify that item text and stemmed item text are recorded.\n      items.forEach((item, index) => {\n        assert.equal(tokenizer.items[index], items[index]);\n        assert.equal(tokenizer.stemmedItems[index], items[index]);\n      });\n\n      // Verify that posting lists are correct.\n      const terms = ['a', 'b', 'c', 'd', 'e', 'f'];\n      const expectedPostings = [\n        [0],     // a\n        [0, 1],  // b\n        [0, 1],  // c\n        [1, 2],  // d\n        [2],     // e\n        [2]      // f\n      ];\n\n      const observedPostings =\n          terms.map((term) => tokenizer.postings[tokenizer.hashTerm(term)]);\n      assert.deepEqual(observedPostings, expectedPostings);\n\n      // Verify that term frequencies are correct.\n      const expectedFrequencies = [\n        1,  // a\n        2,  // b\n        2,  // c\n        2,  // d\n        1,  // e\n        1   // f\n      ];\n      const observedFrequencies = terms.map(\n          (term) => tokenizer.hashToFrequency[tokenizer.hashTerm(term)]);\n      assert.deepEqual(observedFrequencies, expectedFrequencies);\n    });\n  });\n\n  describe('#stemTerm', () => {\n    it('should apply the Snowball English Stemmer', () => {\n      const badWords = new Set([]);\n      const tokenizer = new Tokenizer(badWords);\n      const input = 'sauce chocolate milkshake hamburger value cheese creamy';\n      const terms = input.split(' ');\n      const stemmed = terms.map((term) => tokenizer.stemTerm(term));\n      const observed = stemmed.join(' ');\n      const expected = 'sauc chocol milkshak hamburg valu chees creami';\n      assert.equal(observed, expected);\n    });\n  });\n});\n"]}